<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://physics-morris.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://physics-morris.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-23T00:06:55+00:00</updated><id>https://physics-morris.github.io/feed.xml</id><title type="html">Yu-Chao Huang (Morris)</title><subtitle>Yu-Chao Huang personal website. </subtitle><entry><title type="html">2024 Machine Learning Paper List</title><link href="https://physics-morris.github.io/blog/2024/paper/" rel="alternate" type="text/html" title="2024 Machine Learning Paper List"/><published>2024-12-22T00:00:00+00:00</published><updated>2024-12-22T00:00:00+00:00</updated><id>https://physics-morris.github.io/blog/2024/paper</id><content type="html" xml:base="https://physics-morris.github.io/blog/2024/paper/"><![CDATA[<p>Below are some interesting paper I read in 2024. Feel free to leave a comment or email me to share or suggest more exciting papers to yuchaohuang [at] g [dot] ntu [dot] edu!</p> <hr/> <h2 id="theoretical-works"><strong>Theoretical Works</strong></h2> <ul> <li> <p><strong>How Transformers Learn Causal Structure with Gradient Descent</strong><br/> Eshaan Nichani, Alex Damian, Jason D. Lee (2024).<br/> <em>arXiv preprint, arXiv:2402.14735.</em><br/> <a href="https://arxiv.org/abs/2402.14735">Link to Paper</a></p> </li> <li> <p><strong>Provably Learning a Multi-Head Attention Layer</strong><br/> Sitan Chen, Yuanzhi Li (2024).<br/> <em>arXiv preprint, arXiv:2402.04084.</em><br/> <a href="https://arxiv.org/abs/2402.04084">Link to Paper</a></p> </li> <li> <p><strong>Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality</strong><br/> Tri Dao, Albert Gu (2024).<br/> <em>arXiv preprint, arXiv:2405.21060.</em><br/> <a href="https://arxiv.org/abs/2405.21060">Link to Paper</a></p> </li> <li> <p><strong>A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration</strong><br/> Yingqian Cui, Pengfei He, Xianfeng Tang, Qi He, Chen Luo, Jiliang Tang, Yue Xing (2024).<br/> <em>arXiv preprint, arXiv:2410.16540.</em><br/> <a href="https://arxiv.org/abs/2410.16540">Link to Paper</a></p> </li> </ul> <h2 id="diffusion-model"><strong>Diffusion Model</strong></h2> <ul> <li> <p><strong>Slight Corruption in Pre-training Data Makes Better Diffusion Models</strong><br/> Hao Chen, Yujin Han, Diganta Misra, Xiang Li, Kai Hu, Difan Zou, Masashi Sugiyama, Jindong Wang, Bhiksha Raj (2024).<br/> <em>arXiv preprint, arXiv:2405.20494.</em><br/> <a href="https://arxiv.org/abs/2405.20494">Link to Paper</a></p> </li> <li> <p><strong>Learning Diffusion at Lightspeed</strong><br/> Antonio Terpin, Nicolas Lanzetti, Martín Gadea, Florian Dörfler (2024).<br/> <em>arXiv preprint, arXiv:2406.12616.</em><br/> <a href="https://arxiv.org/abs/2406.12616">Link to Paper</a></p> </li> <li> <p><strong>Generalized Schrödinger Bridge Matching</strong><br/> Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos A. Theodorou, Ricky TQ Chen (2023).<br/> <em>arXiv preprint, arXiv:2310.02233.</em><br/> <a href="https://arxiv.org/abs/2310.02233">Link to Paper</a></p> </li> <li> <p><strong>A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training</strong><br/> Kai Wang, Mingjia Shi, Yukun Zhou, Zekai Li, Zhihang Yuan, Yuzhang Shang, Xiaojiang Peng, Hanwang Zhang, Yang You (2024).<br/> <em>arXiv preprint, arXiv:2405.17403.</em><br/> <a href="https://arxiv.org/abs/2405.17403">Link to Paper</a></p> </li> <li> <p><strong>Diffusion Forcing: Next-Token Prediction Meets Full-Sequence Diffusion</strong><br/> Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, Vincent Sitzmann (2024).<br/> <em>arXiv preprint, arXiv:2407.01392.</em><br/> <a href="https://arxiv.org/abs/2407.01392">Link to Paper</a></p> </li> </ul> <h2 id="foundation-model"><strong>Foundation Model</strong></h2> <ul> <li> <p><strong>Evaluating Quantized Large Language Models</strong><br/> Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang (2024).<br/> <em>arXiv preprint, arXiv:2402.18158.</em><br/> <a href="https://arxiv.org/abs/2402.18158">Link to Paper</a></p> </li> <li> <p><strong>scGPT: Toward Building a Foundation Model for Single-Cell Multi-Omics Using Generative AI</strong><br/> Haotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, Bo Wang (2024).<br/> <em>Nature Methods, 1–11.</em><br/> <a href="https://www.nature.com/articles/s41592-024-01723-4">Link to Paper</a></p> </li> <li> <p><strong>Thinking LLMs: General Instruction Following with Thought Generation</strong><br/> Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar (2024).<br/> <em>arXiv preprint, arXiv:2410.10630.</em><br/> <a href="https://arxiv.org/abs/2410.10630">Link to Paper</a></p> </li> <li> <p><strong>LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens</strong><br/> Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, Mao Yang (2024).<br/> <em>arXiv preprint, arXiv:2402.13753.</em><br/> <a href="https://arxiv.org/abs/2402.13753">Link to Paper</a></p> </li> <li> <p><strong>Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling</strong><br/> Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, Mehran Kazemi (2024).<br/> <em>arXiv preprint, arXiv:2408.16737.</em><br/> <a href="https://arxiv.org/abs/2408.16737">Link to Paper</a></p> </li> <li> <p><strong>Show-o: One Single Transformer to Unify Multimodal Understanding and Generation</strong><br/> Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, Mike Zheng Shou (2024).<br/> <em>arXiv preprint, arXiv:2408.12528.</em><br/> <a href="https://arxiv.org/abs/2408.12528">Link to Paper</a></p> </li> <li> <p><strong>Unified Training of Universal Time Series Forecasting Transformers</strong><br/> Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, Doyen Sahoo (2024).<br/> <em>arXiv preprint, arXiv:2402.02592.</em><br/> <a href="https://arxiv.org/abs/2402.02592">Link to Paper</a></p> </li> <li> <p><strong>Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts</strong><br/> Xu Liu, Juncheng Liu, Gerald Woo, Taha Aksu, Yuxuan Liang, Roger Zimmermann, Chenghao Liu, Silvio Savarese, Caiming Xiong, Doyen Sahoo (2024).<br/> <em>arXiv preprint, arXiv:2410.10469.</em><br/> <a href="https://arxiv.org/abs/2410.10469">Link to Paper</a></p> </li> <li> <p><strong>A Decoder-Only Foundation Model for Time-Series Forecasting</strong><br/> Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou (2023).<br/> <em>arXiv preprint, arXiv:2310.10688.</em><br/> <a href="https://arxiv.org/abs/2310.10688">Link to Paper</a></p> </li> <li> <p><strong>Chronos: Learning the Language of Time Series</strong><br/> Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, Yuyang Wang (2024).<br/> <em>arXiv preprint, arXiv:2403.07815.</em><br/> <a href="https://arxiv.org/abs/2403.07815">Link to Paper</a></p> </li> <li> <p><strong>Cell2Sentence: Teaching Large Language Models the Language of Biology</strong><br/> Daniel Levine, Syed Asad Rizvi, Sacha Lévy, Nazreen Pallikkavaliyaveetil, David Zhang, Xingyu Chen, Sina Ghadermarzi, Ruiming Wu, Zihe Zheng, Ivan Vrkic, et al. (2023).<br/> <em>BioRxiv, Cold Spring Harbor Laboratory.</em><br/> <a href="https://www.biorxiv.org/content/10.1101/2023.09.11.557287v4">Link to Paper</a></p> </li> </ul> <h2 id="transformer"><strong>Transformer</strong></h2> <ul> <li> <p><strong>ngpt: Normalized Transformer with Representation Learning on the Hypersphere</strong><br/> Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg (2024).<br/> <em>arXiv preprint, arXiv:2410.01131.</em><br/> <a href="https://arxiv.org/abs/2410.01131">Link to Paper</a></p> </li> <li> <p><strong>Differential Transformer</strong><br/> Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei (2024).<br/> <em>arXiv preprint, arXiv:2410.05258.</em><br/> <a href="https://arxiv.org/abs/2410.05258">Link to Paper</a></p> </li> </ul> <h2 id="misc"><strong>Misc</strong></h2> <ul> <li> <p><strong>Theory, Analysis, and Best Practices for Sigmoid Self-Attention</strong><br/> Jason Ramapuram, Federico Danieli, Eeshan Dhekane, Floris Weers, Dan Busbridge, Pierre Ablin, Tatiana Likhomanenko, Jagrit Digani, Zijin Gu, Amitis Shidani, et al. (2024).<br/> <em>arXiv preprint, arXiv:2409.04431.</em><br/> <a href="https://arxiv.org/abs/2409.04431">Link to Paper</a></p> </li> <li> <p><strong>De Novo Design of High-Affinity Protein Binders with AlphaProteo</strong><br/> Vinicius Zambaldi, David La, Alexander E. Chu, Harshnira Patani, Amy E. Danson, Tristan O.C. Kwan, Thomas Frerix, Rosalia G. Schneider, David Saxton, Ashok Thillaisundaram, et al. (2024).<br/> <em>arXiv preprint, arXiv:2409.08022.</em><br/> <a href="https://arxiv.org/abs/2409.08022">Link to Paper</a></p> </li> <li> <p><strong>Learning to (Learn at Test Time): RNNs with Expressive Hidden States</strong><br/> Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. (2024).<br/> <em>arXiv preprint, arXiv:2407.04620.</em><br/> <a href="https://arxiv.org/abs/2407.04620">Link to Paper</a></p> </li> <li> <p><strong>The Unbearable Slowness of Being</strong><br/> Jieyu Zheng, Markus Meister (2024).<br/> <em>arXiv preprint, arXiv:2408.10234.</em><br/> <a href="https://arxiv.org/abs/2408.10234">Link to Paper</a></p> </li> <li> <p><strong>Discrete Flow Matching</strong><br/> Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, Yaron Lipman (2024).<br/> <em>arXiv preprint, arXiv:2407.15595.</em><br/> <a href="https://arxiv.org/abs/2407.15595">Link to Paper</a></p> </li> </ul>]]></content><author><name></name></author><category term="Machine"/><category term="Learning"/><category term="Note,"/><category term="Paper"/><summary type="html"><![CDATA[A list of interesting ML papers from 2024]]></summary></entry><entry><title type="html">Continuous Ranked Probability Score (CRPS)</title><link href="https://physics-morris.github.io/blog/2024/CRPS/" rel="alternate" type="text/html" title="Continuous Ranked Probability Score (CRPS)"/><published>2024-12-17T00:00:00+00:00</published><updated>2024-12-17T00:00:00+00:00</updated><id>https://physics-morris.github.io/blog/2024/CRPS</id><content type="html" xml:base="https://physics-morris.github.io/blog/2024/CRPS/"><![CDATA[<h2 id="what-is-continuous-ranked-probability-score-crps-">What is Continuous Ranked Probability Score (CRPS) ?</h2> <p>How to evaluate probablistic forcasting is good or not? Continuous Ranked Probability Score (CRPS) is a evaluation metrics to quantify this. <d-footnote>Gneiting, T., &amp; Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477), 359-378.</d-footnote></p> <p>Below is an example of probability forcasting of airline passengers <d-footnote>Data from: https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv</d-footnote> where blue line is the ground truth and the red line is the forecase mean with the ribbon is various confidence interval.</p> <center> <div class="l-page"> <iframe src="/assets/plotly/airline_passengers_forecast.html" frameborder="0" scrolling="no" height="400px" width="100%" onerror="this.onerror=null;this.src='/assets/img/NTU-logo-horizontal';"> </iframe> <noscript> <img src="/assets/img/twostream/NTU-logo-horizontal.png" height="300px" width="70%"/> </noscript> </div></center> <h3 id="definition">Definition</h3> <p>The CRPS measures the difference between a predicted <strong>cumulative distribution function</strong> (CDF) \(F(x)\) and the observed value \(y\). It is defined as:</p> \[\text{CRPS}(F, y) = \int_{-\infty}^\infty \left( F(x) - \mathbb{1}(x \geq y) \right)^2 dx.\] <p>The perfect prediction is \(\mathbb{1}(x \geq y)\) where the CDF is \(1\) greater and equal to observation \(y\). The square of difference measure the closeness of prediction density to this perfect density.</p> <h3 id="quantile-loss">Quantile Loss</h3> <p>The quantile loss \(\rho_{\alpha}(u)\) at a quantile level \(\alpha \in [0, 1]\) for a deviation \(u = z - q(\alpha)\) is defined as:</p> \[\rho_{\alpha}(u) = \begin{cases} \alpha \cdot u &amp; \text{if } u \geq 0, \\ (1 - \alpha) \cdot |u| &amp; \text{if } u &lt; 0. \end{cases}\] <p>where \(z\) is the true value, \(q(\alpha)\) is the predicted quantile at level $\alpha$, and \(u = z - q(\alpha)\) is the difference between true and prediction. The intuition make sense if we want to predict 75th quantile, we will penelize \(3\) times for under-estimated than over-estimated, for more example you can refer to here <d-footnote>https://www.kaggle.com/code/vyacheslavefimov/quantile-loss-quantile-regression</d-footnote>.</p> <h3 id="quantile-decomposition-of-crps">Quantile Decomposition of CRPS</h3> <p>We can rewrite CRPS as integral over quantile loss <d-footnote>Gneiting, T., &amp; Ranjan, R. (2011). Comparing density forecasts using threshold-and quantile-weighted scoring rules. Journal of Business &amp; Economic Statistics, 29(3), 411-422.</d-footnote></p> \[\begin{align} \text{CRPS}(F, y) &amp;= \int_{-\infty}^\infty \left( F(x) - \mathbb{1}(x \geq y) \right)^2 dx \\ &amp;= \int_{0}^{1} 2 \rho_{\alpha} \left( z - q(\alpha) \right) d \alpha, \end{align}\] <p>where (2) is the quantile decomposition of the CRPS. The key step in the derivation involves performing a <strong>change of variable</strong> from \(x\) to \(\alpha\) and utilizing the relationship between the quantile function and the cumulative distribution function (CDF). Specifically, the quantile function \(q(\alpha)\) is defined as the inverse of the CDF \(F(x)\): \(\begin{align} q(\alpha) = F^{-1}(\alpha) = \inf \{ x \in \mathbb{R} \mid F(x) \geq \alpha \}. \end{align}\)</p>]]></content><author><name>Yu-Chao Huang</name></author><category term="ML"/><category term="Note"/><summary type="html"><![CDATA[A Note on Continuous Ranked Probability Score (CRPS)]]></summary></entry><entry><title type="html">Test-Time Training with Quantum Auto-Encoder</title><link href="https://physics-morris.github.io/blog/2024/QTTT/" rel="alternate" type="text/html" title="Test-Time Training with Quantum Auto-Encoder"/><published>2024-11-11T10:30:00+00:00</published><updated>2024-11-11T10:30:00+00:00</updated><id>https://physics-morris.github.io/blog/2024/QTTT</id><content type="html" xml:base="https://physics-morris.github.io/blog/2024/QTTT/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="ML"/><category term="Paper"/><summary type="html"><![CDATA[blog post for our paper tackling distribution shift and noisy quantum circuits]]></summary></entry><entry><title type="html">Finite Difference in Time Domain in 1D</title><link href="https://physics-morris.github.io/blog/2022/1d-fdtd/" rel="alternate" type="text/html" title="Finite Difference in Time Domain in 1D"/><published>2022-12-17T00:00:00+00:00</published><updated>2022-12-17T00:00:00+00:00</updated><id>https://physics-morris.github.io/blog/2022/1d-fdtd</id><content type="html" xml:base="https://physics-morris.github.io/blog/2022/1d-fdtd/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="Physics"/><category term="Simulation"/><summary type="html"><![CDATA[A note on Finite Difference in Time Domain in 1D]]></summary></entry><entry><title type="html">Planar Gravitational 3-Body Problem</title><link href="https://physics-morris.github.io/blog/2022/3body/" rel="alternate" type="text/html" title="Planar Gravitational 3-Body Problem"/><published>2022-12-17T00:00:00+00:00</published><updated>2022-12-17T00:00:00+00:00</updated><id>https://physics-morris.github.io/blog/2022/3body</id><content type="html" xml:base="https://physics-morris.github.io/blog/2022/3body/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="Physics"/><category term="Simulation"/><summary type="html"><![CDATA[Numerically Solving Planar Gravitational 3-Body Problem]]></summary></entry><entry><title type="html">Ferromagnetic Ising Model on a Square Lattice</title><link href="https://physics-morris.github.io/blog/2022/Ferromagnetic-Ising/" rel="alternate" type="text/html" title="Ferromagnetic Ising Model on a Square Lattice"/><published>2022-12-17T00:00:00+00:00</published><updated>2022-12-17T00:00:00+00:00</updated><id>https://physics-morris.github.io/blog/2022/Ferromagnetic-Ising</id><content type="html" xml:base="https://physics-morris.github.io/blog/2022/Ferromagnetic-Ising/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="Physics"/><category term="Simulation"/><summary type="html"><![CDATA[Ferromagnetic Ising Model]]></summary></entry><entry><title type="html">Fibonocci Numbers</title><link href="https://physics-morris.github.io/blog/2022/fibonocci/" rel="alternate" type="text/html" title="Fibonocci Numbers"/><published>2022-12-17T00:00:00+00:00</published><updated>2022-12-17T00:00:00+00:00</updated><id>https://physics-morris.github.io/blog/2022/fibonocci</id><content type="html" xml:base="https://physics-morris.github.io/blog/2022/fibonocci/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="Numerical"/><category term="Computation"/><summary type="html"><![CDATA[A note on Fibonocci Numbers]]></summary></entry><entry><title type="html">Gaussian Random Variable and CLT</title><link href="https://physics-morris.github.io/blog/2022/gaussian-clt/" rel="alternate" type="text/html" title="Gaussian Random Variable and CLT"/><published>2022-12-17T00:00:00+00:00</published><updated>2022-12-17T00:00:00+00:00</updated><id>https://physics-morris.github.io/blog/2022/gaussian-clt</id><content type="html" xml:base="https://physics-morris.github.io/blog/2022/gaussian-clt/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="Physics"/><category term="Simulation"/><summary type="html"><![CDATA[A note on Gaussian Random Variable and CLT]]></summary></entry><entry><title type="html">Horner’s method</title><link href="https://physics-morris.github.io/blog/2022/horner/" rel="alternate" type="text/html" title="Horner’s method"/><published>2022-12-17T00:00:00+00:00</published><updated>2022-12-17T00:00:00+00:00</updated><id>https://physics-morris.github.io/blog/2022/horner</id><content type="html" xml:base="https://physics-morris.github.io/blog/2022/horner/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="Physics"/><category term="Simulation"/><summary type="html"><![CDATA[A note on Horner’s method]]></summary></entry><entry><title type="html">Ideal Random Walk</title><link href="https://physics-morris.github.io/blog/2022/ideal-rw/" rel="alternate" type="text/html" title="Ideal Random Walk"/><published>2022-12-17T00:00:00+00:00</published><updated>2022-12-17T00:00:00+00:00</updated><id>https://physics-morris.github.io/blog/2022/ideal-rw</id><content type="html" xml:base="https://physics-morris.github.io/blog/2022/ideal-rw/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><category term="Physics"/><category term="Simulation"/><summary type="html"><![CDATA[Simulation of Ideal Random Walk]]></summary></entry></feed>